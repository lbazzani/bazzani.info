# Architettura Sistema RL - Sensor Optimization

Diagrammi e spiegazioni dell'architettura del sistema.

---

## 1. Overview Generale

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ                     RL TRAINING SYSTEM                           โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโค
โ                                                                  โ
โ  โโโโโโโโโโโโโโโโ         โโโโโโโโโโโโโโโโ                     โ
โ  โ Environment  โโโโโโโโโโบโ  RL Agent    โ                     โ
โ  โ (Gymnasium)  โ         โ  (DQN/AC)    โ                     โ
โ  โโโโโโโโฌโโโโโโโโ         โโโโโโโโฌโโโโโโโโ                     โ
โ         โ                        โ                              โ
โ         โ state, reward          โ action                       โ
โ         โ                        โ                              โ
โ         โผ                        โผ                              โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ  โ         Training Loop                     โ                  โ
โ  โ  - Experience Collection                  โ                  โ
โ  โ  - Replay Buffer                          โ                  โ
โ  โ  - Network Updates                        โ                  โ
โ  โ  - Checkpointing                          โ                  โ
โ  โโโโโโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ                 โ                                                โ
โ                 โผ                                                โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ  โ   Monitoring & Logging                    โ                  โ
โ  โ   - TensorBoard                           โ                  โ
โ  โ   - Metrics Tracking                      โ                  โ
โ  โ   - Model Checkpoints                     โ                  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                 โ
โ                                                                  โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

---

## 2. Environment Architecture

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ             SensorOptimizationEnv (Gymnasium)                     โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโค
โ                                                                   โ
โ  INPUT: Configuration                                             โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                    โ
โ  โ โข Sensors: [{x, y, z, type}, ...]       โ                    โ
โ  โ โข Constraints: [{x, y, z}, ...]         โ                    โ
โ  โ โข Space: (unitsX, unitsY, unitsZ)       โ                    โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                    โ
โ                    โผ                                              โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                    โ
โ  โ        State Encoder                     โ                    โ
โ  โ  - Sensor distribution features          โ                    โ
โ  โ  - Constraint density map                โ                    โ
โ  โ  - Junction box positions                โ                    โ
โ  โ  - Coverage metrics                      โ                    โ
โ  โ  โ 512-dim feature vector                โ                    โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                    โ
โ                    โผ                                              โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                    โ
โ  โ        Action Processor                  โ                    โ
โ  โ  Input: [x, y, z, type, ports]          โ                    โ
โ  โ  - Denormalize coordinates              โ                    โ
โ  โ  - Discretize ports (6/12/24)           โ                    โ
โ  โ  - Create junction box                   โ                    โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                    โ
โ                    โผ                                              โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                    โ
โ  โ        Reward Calculator                 โ                    โ
โ  โ  Components:                             โ                    โ
โ  โ  โข Coverage: +100 per sensor             โ                    โ
โ  โ  โข Cable: -0.5 * length                  โ                    โ
โ  โ  โข Boxes: -10 per box                    โ                    โ
โ  โ  โข Violations: -1000                     โ                    โ
โ  โ  โข Completion: +1000                     โ                    โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                    โ
โ                    โผ                                              โ
โ  OUTPUT: (next_state, reward, done, info)                        โ
โ                                                                   โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

### State Encoding Detail

```
State Vector (512-dim):
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ [0:24]    Sensor Type 0 features (8 per type ร 3)   โ
โ           - Centroid (x, y, z) normalized            โ
โ           - Std dev (x, y, z) normalized             โ
โ           - Count ratio                              โ
โ           - Unconnected ratio                        โ
โ                                                      โ
โ [24:28]   Constraint features                       โ
โ           - Density                                  โ
โ           - Centroid (x, y, z)                      โ
โ                                                      โ
โ [28:33]   Junction box features                     โ
โ           - Count                                    โ
โ           - Centroid (x, y, z)                      โ
โ           - Avg ports                                โ
โ                                                      โ
โ [33:39]   Coverage metrics                          โ
โ           - Overall coverage                         โ
โ           - Connected ratio                          โ
โ           - Unconnected per type                     โ
โ                                                      โ
โ [39:43]   Space dimensions                          โ
โ           - unitsX, unitsY, unitsZ normalized       โ
โ           - Step progress                            โ
โ                                                      โ
โ [43:512]  Padding (zeros)                           โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

---

## 3. DQN Agent Architecture

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ                       DQN Agent                                 โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโค
โ                                                                 โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ  โ         Q-Network (Online)              โ                  โ
โ  โ                                          โ                  โ
โ  โ  Input: State (512-dim)                 โ                  โ
โ  โ    โผ                                     โ                  โ
โ  โ  Linear(512 โ 256) + ReLU + Dropout     โ                  โ
โ  โ    โผ                                     โ                  โ
โ  โ  Linear(256 โ 256) + ReLU + Dropout     โ                  โ
โ  โ    โผ                                     โ                  โ
โ  โ  Linear(256 โ 128) + ReLU               โ                  โ
โ  โ    โผ                                     โ                  โ
โ  โ  Linear(128 โ 5)                        โ                  โ
โ  โ    โผ                                     โ                  โ
โ  โ  Output: Q-values (5-dim)               โ                  โ
โ  โ          [x, y, z, type, ports]         โ                  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ                                                                 โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ  โ      Target Network (Frozen)            โ                  โ
โ  โ                                          โ                  โ
โ  โ  Same architecture as Q-Network         โ                  โ
โ  โ  Updated periodically (every N steps)   โ                  โ
โ  โ  Used for stable target calculation     โ                  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ                                                                 โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ  โ      Replay Buffer                      โ                  โ
โ  โ                                          โ                  โ
โ  โ  Stores: (s, a, r, s', done)           โ                  โ
โ  โ  Capacity: 100,000 transitions          โ                  โ
โ  โ  Sampling: Random batch of 64           โ                  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ                                                                 โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ  โ      Epsilon-Greedy Strategy            โ                  โ
โ  โ                                          โ                  โ
โ  โ  if random() < epsilon:                 โ                  โ
โ  โ      action = random()                   โ                  โ
โ  โ  else:                                   โ                  โ
โ  โ      action = argmax Q(s, a)            โ                  โ
โ  โ                                          โ                  โ
โ  โ  epsilon decay: ฮต = max(0.01, ฮต*0.995)  โ                  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                  โ
โ                                                                 โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

### Bellman Update

```
Target Q-value:
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ  Q_target(s, a) = r + ฮณ * max Q_target(s', a') โ
โ                          a'                    โ
โ  where:                                        โ
โ    r = immediate reward                        โ
โ    ฮณ = 0.99 (discount factor)                 โ
โ    s' = next state                             โ
โ    Q_target = target network                   โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

Loss:
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ  L = MSE(Q_online(s, a), Q_target(s, a))      โ
โ                                                โ
โ  Optimize Q_online via gradient descent:       โ
โ    ฮธ โ ฮธ - ฮฑ * โL                             โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

---

## 4. Training Loop Flow

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ                   TRAINING EPISODE                           โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

    START
      โ
      โผ
  โโโโโโโโโโโโโโโโโ
  โ env.reset()   โ  Generate random configuration
  โ               โ  - Random sensor positions
  โ               โ  - Random constraints
  โโโโโโโโโฌโโโโโโโโ
          โ
          โผ
  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
  โ               Episode Loop (max 50 steps)              โ
  โ                                                        โ
  โ  โโโโโโโโโโโโโโโโโโโ                                  โ
  โ  โ 1. Select Actionโ                                  โ
  โ  โ   ฮต-greedy      โ                                  โ
  โ  โโโโโโโโโโฌโโโโโโโโโ                                  โ
  โ           โ                                            โ
  โ           โผ                                            โ
  โ  โโโโโโโโโโโโโโโโโโโ                                  โ
  โ  โ 2. Env Step     โ                                  โ
  โ  โ   Place junctionโ                                  โ
  โ  โ   box           โ                                  โ
  โ  โโโโโโโโโโฌโโโโโโโโโ                                  โ
  โ           โ                                            โ
  โ           โผ                                            โ
  โ  โโโโโโโโโโโโโโโโโโโ                                  โ
  โ  โ 3. Calculate    โ                                  โ
  โ  โ   Reward        โ                                  โ
  โ  โโโโโโโโโโฌโโโโโโโโโ                                  โ
  โ           โ                                            โ
  โ           โผ                                            โ
  โ  โโโโโโโโโโโโโโโโโโโ                                  โ
  โ  โ 4. Store in     โ                                  โ
  โ  โ   Replay Buffer โ                                  โ
  โ  โโโโโโโโโโฌโโโโโโโโโ                                  โ
  โ           โ                                            โ
  โ           โผ                                            โ
  โ  โโโโโโโโโโโโโโโโโโโ                                  โ
  โ  โ 5. Train Agent  โ                                  โ
  โ  โ   Sample batch  โ                                  โ
  โ  โ   Update Q-net  โ                                  โ
  โ  โโโโโโโโโโฌโโโโโโโโโ                                  โ
  โ           โ                                            โ
  โ           โผ                                            โ
  โ     Done? โโYesโโโบ End Episode                        โ
  โ       โ                                                โ
  โ       No                                               โ
  โ       โ                                                โ
  โ       โโโโโโโบ Loop                                    โ
  โ                                                        โ
  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
          โ
          โผ
  โโโโโโโโโโโโโโโโโโโโโ
  โ Update Target Net โ  Every 10 episodes
  โโโโโโโโโฌโโโโโโโโโโโโ
          โ
          โผ
  โโโโโโโโโโโโโโโโโโโโโ
  โ Save Checkpoint   โ  Every 100 episodes
  โโโโโโโโโฌโโโโโโโโโโโโ
          โ
          โผ
  โโโโโโโโโโโโโโโโโโโโโ
  โ Evaluation        โ  Every 50 episodes
  โ - Test on 5 eps   โ
  โ - Log metrics     โ
  โโโโโโโโโฌโโโโโโโโโโโโ
          โ
          โผ
    Next Episode
```

---

## 5. Data Flow

```
Configuration Generation
โโโโโโโโโโโโโโโโโโโโโโโโ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ Random Space: 20-100 units per dimension โ
โ Random Sensors: 10-50 per type           โ
โ Random Constraints: 5-15% of volume      โ
โโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
           โ
           โผ
State Encoding (512-dim)
โโโโโโโโโโโโโโโโโโโโโโโโ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ Features extracted:                       โ
โ โข Spatial distribution                    โ
โ โข Density metrics                         โ
โ โข Coverage stats                          โ
โโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
           โ
           โผ
Agent Decision (5-dim)
โโโโโโโโโโโโโโโโโโโโโโ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ Q-Network forward pass                    โ
โ โข Input: state (512)                      โ
โ โข Output: Q-values (5)                    โ
โ โข Epsilon-greedy selection                โ
โโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
           โ
           โผ
Action Execution
โโโโโโโโโโโโโโโโ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ Denormalize & place junction box:        โ
โ โข x, y, z โ world coordinates             โ
โ โข type โ sensor type ID                   โ
โ โข ports โ 6, 12, or 24                    โ
โโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
           โ
           โผ
Reward Calculation
โโโโโโโโโโโโโโโโโโ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ Components:                               โ
โ โข +100 * new connections                  โ
โ โข -0.5 * cable length                     โ
โ โข -10 per junction box                    โ
โ โข -1000 if constraint violated            โ
โ โข +1000 if all connected                  โ
โโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
           โ
           โผ
Experience Storage
โโโโโโโโโโโโโโโโโโ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ Replay Buffer: (s, a, r, s', done)       โ
โ Capacity: 100,000                         โ
โ Current size: โฌคโฌคโฌคโฌคโฌคโฌคโฌคโฌคโฌคโฌค 45,123          โ
โโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
           โ
           โผ
Batch Sampling & Training
โโโโโโโโโโโโโโโโโโโโโโโโโ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ Sample 64 transitions randomly            โ
โ Compute target Q-values                   โ
โ Backprop through Q-network                โ
โ Gradient clipping (max_norm=1.0)         โ
โ Optimizer step (Adam, lr=1e-4)           โ
โโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
           โ
           โผ
Logging & Checkpointing
โโโโโโโโโโโโโโโโโโโโโโโ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ TensorBoard:                              โ
โ โข Reward per episode                      โ
โ โข Coverage metrics                        โ
โ โข Loss values                             โ
โ Checkpoints:                              โ
โ โข Best model (highest reward)             โ
โ โข Periodic (every 100 episodes)           โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

---

## 6. Model Architecture Details

### Q-Network Layer Dimensions

```
Layer 1: Linear
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
Input:  [batch_size, 512]
Weight: [512, 256]
Bias:   [256]
Output: [batch_size, 256]
ReLU + Dropout(0.2)

Layer 2: Linear
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
Input:  [batch_size, 256]
Weight: [256, 256]
Bias:   [256]
Output: [batch_size, 256]
ReLU + Dropout(0.2)

Layer 3: Linear
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
Input:  [batch_size, 256]
Weight: [256, 128]
Bias:   [128]
Output: [batch_size, 128]
ReLU

Layer 4: Linear (Output)
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
Input:  [batch_size, 128]
Weight: [128, 5]
Bias:   [5]
Output: [batch_size, 5]  โ Q-values for action

Total Parameters: 512*256 + 256*256 + 256*128 + 128*5 + biases
                โ 229,000 parameters
```

---

## 7. File Dependencies

```
src/
โโโ environment.py
โ   โโโ Dependencies: gymnasium, numpy
โ
โโโ agent.py
โ   โโโ Dependencies: torch, numpy
โ   โโโ Uses: ReplayBuffer, QNetwork classes
โ
โโโ training.py
โ   โโโ Dependencies: torch, tensorboard, argparse
โ   โโโ Imports: environment.py, agent.py
โ
โโโ evaluate.py
    โโโ Dependencies: torch, matplotlib, seaborn
    โโโ Imports: environment.py, agent.py

Flow:
training.py
    โ
    โโโโบ Creates Environment (environment.py)
    โโโโบ Creates Agent (agent.py)
    โโโโบ Runs training loop
    โโโโบ Logs to TensorBoard

evaluate.py
    โ
    โโโโบ Loads trained Agent (agent.py)
    โโโโบ Creates Environment (environment.py)
    โโโโบ Runs evaluation episodes
    โโโโบ Generates plots
```

---

## 8. Optimization Pipeline

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ                 FROM TRAINING TO PRODUCTION                 โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

1. Training (Python/PyTorch)
   โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
   โ python src/training.py              โ
   โ   โ models/checkpoints/best.pth     โ
   โโโโโโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโ
                  โ
                  โผ
2. Evaluation (Python)
   โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
   โ python src/evaluate.py              โ
   โ   โ Verify performance              โ
   โ   โ Compare with baseline           โ
   โโโโโโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโ
                  โ
                  โผ
3. Export (ONNX โ TensorFlow)
   โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
   โ python src/export_model.py          โ
   โ   โ model.onnx                      โ
   โ   โ model.pb (TensorFlow)           โ
   โโโโโโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโ
                  โ
                  โผ
4. Convert (TensorFlow.js)
   โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
   โ tensorflowjs_converter              โ
   โ   โ model.json + weights            โ
   โโโโโโโโโโโโโโโโฌโโโโโโโโโโโโโโโโโโโโโโโ
                  โ
                  โผ
5. Integration (React/Next.js)
   โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
   โ import * as tf from '@tensorflow/tfjs'โ
   โ const model = await tf.loadLayersModel()โ
   โ const prediction = model.predict(state) โ
   โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

6. Production Inference
   โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
   โ User configures sensors             โ
   โ   โ                                 โ
   โ State encoding (JS)                 โ
   โ   โ                                 โ
   โ Model inference (~10-50ms)          โ
   โ   โ                                 โ
   โ Action decoding                     โ
   โ   โ                                 โ
   โ Place junction boxes                โ
   โ   โ                                 โ
   โ Visualize 3D                        โ
   โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

---

## 9. Memory & Performance

### Training Memory Usage

```
Component                   Memory
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
Q-Network (parameters)      ~1 MB
Target Network              ~1 MB
Replay Buffer (100k)        ~200 MB
  - State: 512 * 4 bytes
  - Action: 5 * 4 bytes
  - Reward: 4 bytes
  - Next state: 512 * 4 bytes
  - Done: 1 byte
Gradients & Optimizer       ~5 MB
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
Total                       ~210 MB
```

### Inference Performance

```
Operation                   Time (CPU)
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
State encoding              ~1 ms
Network forward pass        ~5 ms
Action decoding             ~1 ms
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
Total per action            ~7 ms

Episode (20 steps)          ~140 ms
```

---

## 10. Extension Points

Per estendere il sistema:

### Aggiungere Nuovo Algoritmo

```python
# src/agent.py

class NewRLAgent:
    def __init__(self, ...):
        # Initialize networks
        pass

    def select_action(self, state):
        # Action selection logic
        pass

    def update(self, batch):
        # Training logic
        pass
```

### Modificare Reward Function

```python
# src/environment.py

def _calculate_reward(self, jbox):
    # Add new reward component
    efficiency_bonus = ...

    total_reward = (
        coverage_reward +
        -cable_penalty +
        efficiency_bonus  # โ New
    )
    return total_reward
```

### Aggiungere Nuove Features allo State

```python
# src/environment.py

def _get_observation(self):
    features = []

    # Existing features
    features.extend(sensor_features)
    features.extend(constraint_features)

    # New features
    features.extend(custom_features)  # โ Add here

    return np.array(features)
```

---

## Conclusione Architettura

Sistema **modulare, estensibile e production-ready**:

โ Environment Gymnasium standard
โ Agent architecture flessibile
โ Training pipeline robusto
โ Monitoring completo
โ Export per produzione
โ Documentazione dettagliata

Pronto per training e deployment! ๐
