{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Sensor Optimization - Quick Start\n",
    "\n",
    "Notebook per testare rapidamente l'environment e l'agent RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from environment import SensorOptimizationEnv\n",
    "from agent import DQNAgent, ActorCriticAgent\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea environment\n",
    "env = SensorOptimizationEnv(\n",
    "    unitsX=30,\n",
    "    unitsY=30,\n",
    "    unitsZ=30,\n",
    "    num_sensor_types=3,\n",
    "    sensors_per_type=[15, 10, 8],\n",
    "    constraint_ratio=0.1\n",
    ")\n",
    "\n",
    "print(f\"Observation space: {env.observation_space.shape}\")\n",
    "print(f\"Action space: {env.action_space.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset e visualizza stato iniziale\n",
    "obs, info = env.reset()\n",
    "print(f\"Initial observation shape: {obs.shape}\")\n",
    "print(f\"Info: {info}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test azione random\n",
    "action = env.action_space.sample()\n",
    "print(f\"Random action: {action}\")\n",
    "\n",
    "next_obs, reward, terminated, truncated, step_info = env.step(action)\n",
    "print(f\"\\nReward: {reward:.2f}\")\n",
    "print(f\"Coverage: {step_info['coverage']:.2%}\")\n",
    "print(f\"Num boxes: {step_info['num_boxes']}\")\n",
    "print(f\"Cable length: {step_info['total_cable_length']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run episodio con random agent\n",
    "def run_random_episode(env, max_steps=50):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    rewards = []\n",
    "    coverages = []\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = env.action_space.sample()\n",
    "        next_obs, reward, terminated, truncated, step_info = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        rewards.append(reward)\n",
    "        coverages.append(step_info['coverage'])\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    return episode_reward, rewards, coverages, step_info\n",
    "\n",
    "# Run\n",
    "total_reward, rewards, coverages, final_info = run_random_episode(env)\n",
    "\n",
    "print(f\"Episode finished!\")\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "print(f\"Final coverage: {final_info['coverage']:.2%}\")\n",
    "print(f\"Final cable length: {final_info['total_cable_length']:.2f}\")\n",
    "print(f\"Num boxes: {final_info['num_boxes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(rewards)\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].set_title('Rewards per Step')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(coverages)\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Coverage')\n",
    "axes[1].set_title('Coverage over Time')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea agent\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=256,\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "\n",
    "print(f\"Agent created on device: {agent.device}\")\n",
    "print(f\"Q-network: {agent.q_network}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test action selection\n",
    "state, _ = env.reset()\n",
    "action = agent.select_action(state, explore=True)\n",
    "print(f\"Selected action: {action}\")\n",
    "print(f\"Action shape: {action.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mini Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini training (pochi episodi per test)\n",
    "num_episodes = 50\n",
    "max_steps = 30\n",
    "\n",
    "episode_rewards = []\n",
    "episode_coverages = []\n",
    "losses = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action\n",
    "        action = agent.select_action(state, explore=True)\n",
    "        \n",
    "        # Step\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store transition\n",
    "        agent.store_transition(state, action, reward, next_state, terminated or truncated)\n",
    "        \n",
    "        # Train\n",
    "        loss = agent.train_step()\n",
    "        if loss is not None:\n",
    "            losses.append(loss)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % 5 == 0:\n",
    "        agent.update_target_network()\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_coverages.append(info['coverage'])\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}: Reward = {episode_reward:.2f}, Coverage = {info['coverage']:.2%}, Epsilon = {agent.epsilon:.3f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Rewards\n",
    "axes[0].plot(episode_rewards)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Total Reward')\n",
    "axes[0].set_title('Training Rewards')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Coverages\n",
    "axes[1].plot(episode_coverages)\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Coverage')\n",
    "axes[1].set_title('Coverage over Training')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Losses\n",
    "axes[2].plot(losses)\n",
    "axes[2].set_xlabel('Training Step')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_title('Training Loss')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate su 10 episodi\n",
    "eval_rewards = []\n",
    "eval_coverages = []\n",
    "\n",
    "for _ in range(10):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state, explore=False)  # Greedy\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    eval_rewards.append(episode_reward)\n",
    "    eval_coverages.append(info['coverage'])\n",
    "\n",
    "print(f\"Evaluation Results:\")\n",
    "print(f\"  Mean Reward: {np.mean(eval_rewards):.2f} Â± {np.std(eval_rewards):.2f}\")\n",
    "print(f\"  Mean Coverage: {np.mean(eval_coverages):.2%}\")\n",
    "print(f\"  Success Rate: {sum(c >= 0.99 for c in eval_coverages) / len(eval_coverages):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save/Load Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save agent\n",
    "agent.save('../models/test_agent.pth')\n",
    "print(\"Agent saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent\n",
    "new_agent = DQNAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "new_agent.load('../models/test_agent.pth')\n",
    "print(\"Agent loaded!\")\n",
    "print(f\"Stats: {new_agent.get_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
